{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b8caa3522fc4cbab31e13b5dfc7808d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_574140e4c4bc48c9a171541a02cd0211",
              "IPY_MODEL_35e03ce5090346c9ae602891470fc555",
              "IPY_MODEL_c942c208e72d46568b476bb0f2d75496"
            ],
            "layout": "IPY_MODEL_65881db1db8a4e9c930fab9172d45143"
          }
        },
        "574140e4c4bc48c9a171541a02cd0211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60b913d755b34d638478e30705a2dde1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0856bea36ec148b68522ff9c9eb258d8",
            "value": "100%"
          }
        },
        "35e03ce5090346c9ae602891470fc555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76879f6f2aa54637a7a07faeea2bd684",
            "max": 818322941,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ace3934ec6f4d36a1b3a9e086390926",
            "value": 818322941
          }
        },
        "c942c208e72d46568b476bb0f2d75496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6b7a2243e0c4beca714d99dceec23d6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5966ba6e6f114d8c9d8d1d6b1bd4f4c7",
            "value": " 780M/780M [02:19&lt;00:00, 6.24MB/s]"
          }
        },
        "65881db1db8a4e9c930fab9172d45143": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60b913d755b34d638478e30705a2dde1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0856bea36ec148b68522ff9c9eb258d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76879f6f2aa54637a7a07faeea2bd684": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ace3934ec6f4d36a1b3a9e086390926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6b7a2243e0c4beca714d99dceec23d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5966ba6e6f114d8c9d8d1d6b1bd4f4c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guidopanzetta/DizBackup/blob/master/Progetto_YOLOv5_WeaponDet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6MPjfT5NrKQ"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "  <a href=\"https://ultralytics.com/yolov5\" target=\"_blank\">\n",
        "    <img width=\"1024\", src=\"https://github.com/ultralytics/assets/raw/master/yolov5/v62/splash_notebook.png\"></a>\n",
        "\n",
        "\n",
        "<br>\n",
        "  <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a>\n",
        "  <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "  <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n",
        "<br>\n",
        "\n",
        "This <a href=\"https://github.com/ultralytics/yolov5\">YOLOv5</a> ðŸš€ notebook by <a href=\"https://ultralytics.com\">Ultralytics</a> presents simple train, validate and predict examples to help start your AI adventure.<br>See <a href=\"https://github.com/ultralytics/yolov5/issues/new/choose\">GitHub</a> for community support or <a href=\"https://ultralytics.com/contact\">contact us</a> for professional support.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Clone GitHub [repository](https://github.com/ultralytics/yolov5), install [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) and check PyTorch and GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6d3bde-d727-497d-d913-827ebc78e944"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ðŸš€ v6.2-178-g799e3d0 Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 37.6/78.2 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd yolov5/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3BGiVPzyTb2",
        "outputId": "48ad5bc6-1633-454e-bb4a-7ce2e06c4d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "y6kjCvH9bjhb",
        "outputId": "ea1c0ec6-2af0-4421-b3c8-ddbba1b50ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1a263b8d-7b69-4dfb-8114-5a8644bb83eb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1a263b8d-7b69-4dfb-8114-5a8644bb83eb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data.zip to data.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q train_data.zip -d ."
      ],
      "metadata": {
        "id": "zG0EXDZHgCHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# 1. Detect\n",
        "\n",
        "`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n",
        "\n",
        "```shell\n",
        "python detect.py --source 0  # webcam\n",
        "                          img.jpg  # image \n",
        "                          vid.mp4  # video\n",
        "                          screen  # screenshot\n",
        "                          path/  # directory\n",
        "                          'path/*.jpg'  # glob\n",
        "                          'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
        "                          'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WDqkrBtAPlMe",
        "outputId": "7d51c8d5-8924-48f9-9213-3d8f084428d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR9ZbuQCH7FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daedbd61-0c2d-4250-f931-40cda3d65772"
      },
      "source": [
        "!python detect.py --weights runs/train/exp/weights/last.pt --img 640 --conf 0.25 --source /content/wea2.mp4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp/weights/last.pt'], source=/content/wea2.mp4, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 ðŸš€ v6.2-178-g799e3d0 Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
            "video 1/1 (1/150) /content/wea2.mp4: 384x640 1 gun, 11.5ms\n",
            "video 1/1 (2/150) /content/wea2.mp4: 384x640 1 gun, 8.5ms\n",
            "video 1/1 (3/150) /content/wea2.mp4: 384x640 1 gun, 8.6ms\n",
            "video 1/1 (4/150) /content/wea2.mp4: 384x640 1 gun, 8.5ms\n",
            "video 1/1 (5/150) /content/wea2.mp4: 384x640 1 gun, 8.5ms\n",
            "video 1/1 (6/150) /content/wea2.mp4: 384x640 1 gun, 8.5ms\n",
            "video 1/1 (7/150) /content/wea2.mp4: 384x640 1 gun, 8.6ms\n",
            "video 1/1 (8/150) /content/wea2.mp4: 384x640 1 gun, 9.0ms\n",
            "video 1/1 (9/150) /content/wea2.mp4: 384x640 1 gun, 8.5ms\n",
            "video 1/1 (10/150) /content/wea2.mp4: 384x640 1 gun, 8.5ms\n",
            "video 1/1 (11/150) /content/wea2.mp4: 384x640 1 gun, 8.6ms\n",
            "video 1/1 (12/150) /content/wea2.mp4: 384x640 1 gun, 8.5ms\n",
            "video 1/1 (13/150) /content/wea2.mp4: 384x640 1 gun, 8.8ms\n",
            "video 1/1 (14/150) /content/wea2.mp4: 384x640 1 gun, 8.5ms\n",
            "video 1/1 (15/150) /content/wea2.mp4: 384x640 1 gun, 8.5ms\n",
            "video 1/1 (16/150) /content/wea2.mp4: 384x640 1 gun, 8.5ms\n",
            "video 1/1 (17/150) /content/wea2.mp4: 384x640 1 gun, 8.0ms\n",
            "video 1/1 (18/150) /content/wea2.mp4: 384x640 1 gun, 7.1ms\n",
            "video 1/1 (19/150) /content/wea2.mp4: 384x640 1 gun, 7.1ms\n",
            "video 1/1 (20/150) /content/wea2.mp4: 384x640 1 gun, 7.6ms\n",
            "video 1/1 (21/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (22/150) /content/wea2.mp4: 384x640 1 gun, 7.3ms\n",
            "video 1/1 (23/150) /content/wea2.mp4: 384x640 1 gun, 7.7ms\n",
            "video 1/1 (24/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (25/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (26/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (27/150) /content/wea2.mp4: 384x640 1 gun, 7.7ms\n",
            "video 1/1 (28/150) /content/wea2.mp4: 384x640 1 gun, 8.7ms\n",
            "video 1/1 (29/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (30/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (31/150) /content/wea2.mp4: 384x640 1 gun, 8.0ms\n",
            "video 1/1 (32/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (33/150) /content/wea2.mp4: 384x640 1 gun, 7.9ms\n",
            "video 1/1 (34/150) /content/wea2.mp4: 384x640 1 gun, 7.6ms\n",
            "video 1/1 (35/150) /content/wea2.mp4: 384x640 1 gun, 13.3ms\n",
            "video 1/1 (36/150) /content/wea2.mp4: 384x640 1 gun, 7.7ms\n",
            "video 1/1 (37/150) /content/wea2.mp4: 384x640 1 gun, 8.3ms\n",
            "video 1/1 (38/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (39/150) /content/wea2.mp4: 384x640 1 gun, 8.2ms\n",
            "video 1/1 (40/150) /content/wea2.mp4: 384x640 1 gun, 7.7ms\n",
            "video 1/1 (41/150) /content/wea2.mp4: 384x640 1 gun, 7.6ms\n",
            "video 1/1 (42/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (43/150) /content/wea2.mp4: 384x640 1 gun, 8.7ms\n",
            "video 1/1 (44/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (45/150) /content/wea2.mp4: 384x640 1 gun, 7.3ms\n",
            "video 1/1 (46/150) /content/wea2.mp4: 384x640 1 gun, 7.7ms\n",
            "video 1/1 (47/150) /content/wea2.mp4: 384x640 1 gun, 7.3ms\n",
            "video 1/1 (48/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (49/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (50/150) /content/wea2.mp4: 384x640 1 gun, 10.9ms\n",
            "video 1/1 (51/150) /content/wea2.mp4: 384x640 1 gun, 7.3ms\n",
            "video 1/1 (52/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (53/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (54/150) /content/wea2.mp4: 384x640 1 gun, 7.6ms\n",
            "video 1/1 (55/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (56/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (57/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (58/150) /content/wea2.mp4: 384x640 1 gun, 8.5ms\n",
            "video 1/1 (59/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (60/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (61/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (62/150) /content/wea2.mp4: 384x640 1 gun, 7.7ms\n",
            "video 1/1 (63/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (64/150) /content/wea2.mp4: 384x640 1 gun, 7.3ms\n",
            "video 1/1 (65/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (66/150) /content/wea2.mp4: 384x640 2 guns, 8.1ms\n",
            "video 1/1 (67/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (68/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (69/150) /content/wea2.mp4: 384x640 1 gun, 7.3ms\n",
            "video 1/1 (70/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (71/150) /content/wea2.mp4: 384x640 (no detections), 7.5ms\n",
            "video 1/1 (72/150) /content/wea2.mp4: 384x640 (no detections), 7.3ms\n",
            "video 1/1 (73/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (74/150) /content/wea2.mp4: 384x640 1 gun, 10.8ms\n",
            "video 1/1 (75/150) /content/wea2.mp4: 384x640 (no detections), 7.2ms\n",
            "video 1/1 (76/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (77/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (78/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (79/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (80/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (81/150) /content/wea2.mp4: 384x640 1 gun, 7.1ms\n",
            "video 1/1 (82/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (83/150) /content/wea2.mp4: 384x640 2 guns, 7.1ms\n",
            "video 1/1 (84/150) /content/wea2.mp4: 384x640 2 guns, 7.5ms\n",
            "video 1/1 (85/150) /content/wea2.mp4: 384x640 2 guns, 7.3ms\n",
            "video 1/1 (86/150) /content/wea2.mp4: 384x640 2 guns, 8.5ms\n",
            "video 1/1 (87/150) /content/wea2.mp4: 384x640 2 guns, 7.7ms\n",
            "video 1/1 (88/150) /content/wea2.mp4: 384x640 1 gun, 7.6ms\n",
            "video 1/1 (89/150) /content/wea2.mp4: 384x640 1 gun, 7.9ms\n",
            "video 1/1 (90/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (91/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (92/150) /content/wea2.mp4: 384x640 1 gun, 8.4ms\n",
            "video 1/1 (93/150) /content/wea2.mp4: 384x640 1 gun, 7.7ms\n",
            "video 1/1 (94/150) /content/wea2.mp4: 384x640 1 gun, 7.3ms\n",
            "video 1/1 (95/150) /content/wea2.mp4: 384x640 2 guns, 7.1ms\n",
            "video 1/1 (96/150) /content/wea2.mp4: 384x640 2 guns, 7.7ms\n",
            "video 1/1 (97/150) /content/wea2.mp4: 384x640 1 gun, 10.3ms\n",
            "video 1/1 (98/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (99/150) /content/wea2.mp4: 384x640 1 gun, 7.3ms\n",
            "video 1/1 (100/150) /content/wea2.mp4: 384x640 1 gun, 7.6ms\n",
            "video 1/1 (101/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (102/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (103/150) /content/wea2.mp4: 384x640 1 gun, 8.0ms\n",
            "video 1/1 (104/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (105/150) /content/wea2.mp4: 384x640 1 gun, 7.8ms\n",
            "video 1/1 (106/150) /content/wea2.mp4: 384x640 1 gun, 7.1ms\n",
            "video 1/1 (107/150) /content/wea2.mp4: 384x640 1 gun, 7.0ms\n",
            "video 1/1 (108/150) /content/wea2.mp4: 384x640 1 gun, 7.6ms\n",
            "video 1/1 (109/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (110/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (111/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (112/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (113/150) /content/wea2.mp4: 384x640 1 gun, 10.2ms\n",
            "video 1/1 (114/150) /content/wea2.mp4: 384x640 1 gun, 10.7ms\n",
            "video 1/1 (115/150) /content/wea2.mp4: 384x640 1 gun, 7.3ms\n",
            "video 1/1 (116/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (117/150) /content/wea2.mp4: 384x640 1 gun, 7.3ms\n",
            "video 1/1 (118/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (119/150) /content/wea2.mp4: 384x640 1 gun, 7.3ms\n",
            "video 1/1 (120/150) /content/wea2.mp4: 384x640 1 gun, 11.3ms\n",
            "video 1/1 (121/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (122/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (123/150) /content/wea2.mp4: 384x640 1 gun, 7.1ms\n",
            "video 1/1 (124/150) /content/wea2.mp4: 384x640 1 gun, 7.3ms\n",
            "video 1/1 (125/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (126/150) /content/wea2.mp4: 384x640 1 gun, 7.1ms\n",
            "video 1/1 (127/150) /content/wea2.mp4: 384x640 1 gun, 7.1ms\n",
            "video 1/1 (128/150) /content/wea2.mp4: 384x640 1 gun, 8.2ms\n",
            "video 1/1 (129/150) /content/wea2.mp4: 384x640 1 gun, 7.1ms\n",
            "video 1/1 (130/150) /content/wea2.mp4: 384x640 1 gun, 7.1ms\n",
            "video 1/1 (131/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (132/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (133/150) /content/wea2.mp4: 384x640 1 gun, 7.6ms\n",
            "video 1/1 (134/150) /content/wea2.mp4: 384x640 1 gun, 7.6ms\n",
            "video 1/1 (135/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (136/150) /content/wea2.mp4: 384x640 1 gun, 8.0ms\n",
            "video 1/1 (137/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (138/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (139/150) /content/wea2.mp4: 384x640 1 gun, 7.1ms\n",
            "video 1/1 (140/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (141/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (142/150) /content/wea2.mp4: 384x640 1 gun, 7.1ms\n",
            "video 1/1 (143/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (144/150) /content/wea2.mp4: 384x640 1 gun, 8.6ms\n",
            "video 1/1 (145/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (146/150) /content/wea2.mp4: 384x640 1 gun, 7.3ms\n",
            "video 1/1 (147/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "video 1/1 (148/150) /content/wea2.mp4: 384x640 1 gun, 7.4ms\n",
            "video 1/1 (149/150) /content/wea2.mp4: 384x640 1 gun, 7.5ms\n",
            "video 1/1 (150/150) /content/wea2.mp4: 384x640 1 gun, 7.2ms\n",
            "Speed: 0.4ms pre-process, 7.8ms inference, 0.8ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp7\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkAzDWJ7cWTr"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img align=\"left\" src=\"https://user-images.githubusercontent.com/26833433/127574988-6a558aa1-d268-44b9-bf6b-62d4c605cc72.jpg\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eq1SMWl6Sfn"
      },
      "source": [
        "# 2. Validate\n",
        "Validate a model's accuracy on the [COCO](https://cocodataset.org/#home) dataset's `val` or `test` splits. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQPtK1QYVaD_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9b8caa3522fc4cbab31e13b5dfc7808d",
            "574140e4c4bc48c9a171541a02cd0211",
            "35e03ce5090346c9ae602891470fc555",
            "c942c208e72d46568b476bb0f2d75496",
            "65881db1db8a4e9c930fab9172d45143",
            "60b913d755b34d638478e30705a2dde1",
            "0856bea36ec148b68522ff9c9eb258d8",
            "76879f6f2aa54637a7a07faeea2bd684",
            "0ace3934ec6f4d36a1b3a9e086390926",
            "d6b7a2243e0c4beca714d99dceec23d6",
            "5966ba6e6f114d8c9d8d1d6b1bd4f4c7"
          ]
        },
        "outputId": "102dabed-bc31-42fe-9133-d9ce28a2c01e"
      },
      "source": [
        "# Download COCO val\n",
        "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n",
        "!unzip -q tmp.zip -d ../datasets && rm tmp.zip  # unzip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/780M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b8caa3522fc4cbab31e13b5dfc7808d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X58w8JLpMnjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf60b1b-b098-4657-c863-584f4c9cf078"
      },
      "source": [
        "# Validate YOLOv5s on COCO val\n",
        "!python val.py --weights yolov5s.pt --data cusom_data.yaml --img 640 --half"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/coco.yaml, weights=['yolov5s.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 ðŸš€ v6.2-56-g30e674b Python-3.7.13 torch-1.12.1+cu113 CUDA:0 (Tesla V100-SXM2-16GB, 16160MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 52.7MB/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/datasets/coco/val2017' images and labels...4952 found, 48 missing, 0 empty, 0 corrupt: 100% 5000/5000 [00:00<00:00, 10509.20it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco/val2017.cache\n",
            "                 Class     Images  Instances          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [00:50<00:00,  3.10it/s]\n",
            "                   all       5000      36335       0.67      0.521      0.566      0.371\n",
            "Speed: 0.1ms pre-process, 1.0ms inference, 1.5ms NMS per image at shape (32, 3, 640, 640)\n",
            "\n",
            "Evaluating pycocotools mAP... saving runs/val/exp/yolov5s_predictions.json...\n",
            "loading annotations into memory...\n",
            "Done (t=0.81s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=5.62s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=77.03s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=14.63s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.374\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.572\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.402\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.489\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.311\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.516\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.566\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.378\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.625\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.724\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2VXXXu74w5"
      },
      "source": [
        "# 3. Train\n",
        "\n",
        "<p align=\"\"><a href=\"https://roboflow.com/?ref=ultralytics\"><img width=\"1000\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png\"/></a></p>\n",
        "Close the active learning loop by sampling images from your inference conditions with the `roboflow` pip package\n",
        "<br><br>\n",
        "\n",
        "Train a YOLOv5s model on the [COCO128](https://www.kaggle.com/ultralytics/coco128) dataset with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`.\n",
        "\n",
        "- **Pretrained [Models](https://github.com/ultralytics/yolov5/tree/master/models)** are downloaded\n",
        "automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases)\n",
        "- **[Datasets](https://github.com/ultralytics/yolov5/tree/master/data)** available for autodownload include: [COCO](https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml), [COCO128](https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml), [VOC](https://github.com/ultralytics/yolov5/blob/master/data/VOC.yaml), [Argoverse](https://github.com/ultralytics/yolov5/blob/master/data/Argoverse.yaml), [VisDrone](https://github.com/ultralytics/yolov5/blob/master/data/VisDrone.yaml), [GlobalWheat](https://github.com/ultralytics/yolov5/blob/master/data/GlobalWheat2020.yaml), [xView](https://github.com/ultralytics/yolov5/blob/master/data/xView.yaml), [Objects365](https://github.com/ultralytics/yolov5/blob/master/data/Objects365.yaml), [SKU-110K](https://github.com/ultralytics/yolov5/blob/master/data/SKU-110K.yaml).\n",
        "- **Training Results** are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n",
        "<br><br>\n",
        "\n",
        "A **Mosaic Dataloader** is used for training which combines 4 images into 1 mosaic.\n",
        "\n",
        "## Train on Custom Data with Roboflow ðŸŒŸ NEW\n",
        "\n",
        "[Roboflow](https://roboflow.com/?ref=ultralytics) enables you to easily **organize, label, and prepare** a high quality dataset with your own custom data. Roboflow also makes it easy to establish an active learning pipeline, collaborate with your team on dataset improvement, and integrate directly into your model building workflow with the `roboflow` pip package.\n",
        "\n",
        "- Custom Training Example: [https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/](https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/?ref=ultralytics)\n",
        "- Custom Training Notebook: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb)\n",
        "<br>\n",
        "\n",
        "<p align=\"\"><a href=\"https://roboflow.com/?ref=ultralytics\"><img width=\"480\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/6152a275ad4b4ac20cd2e21a_roboflow-annotate.gif\"/></a></p>Label images lightning fast (including with model-assisted labeling)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select YOLOv5 ðŸš€ logger {run: 'auto'}\n",
        "logger = 'TensorBoard' #@param ['TensorBoard', 'Comet', 'ClearML', 'W&B']\n",
        "\n",
        "if logger == 'TensorBoard':\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir runs/train\n",
        "elif logger == 'Comet':\n",
        "  %pip install -q comet_ml\n",
        "  import comet_ml; comet_ml.init()\n",
        "elif logger == 'ClearML':\n",
        "  %pip install -q clearml && clearml-init\n",
        "elif logger == 'W&B':\n",
        "  %pip install -q wandb\n",
        "  import wandb; wandb.login()"
      ],
      "metadata": {
        "id": "i3oKtE4g-aNn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcFxRcFdJ_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726e4a6c-0e15-45d1-9997-fd357ac9c01f"
      },
      "source": [
        "# Train YOLOv5s on COCO128 for 3 epochs\n",
        "!python train.py --img 640 --batch 16 --epochs 60 --data custom_data.yaml --weights yolov5s.pt --cache"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=custom_data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=60, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v6.2-178-g799e3d0 Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs in Weights & Biases\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 ðŸš€ in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 47.2MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/train_data/labels/train' images and labels...352 found, 0 missing, 0 empty, 0 corrupt: 100% 352/352 [00:00<00:00, 2297.32it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/train_data/labels/train.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.3GB ram): 100% 352/352 [00:01<00:00, 306.50it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/train_data/labels/val' images and labels...108 found, 0 missing, 0 empty, 0 corrupt: 100% 108/108 [00:00<00:00, 902.98it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/train_data/labels/val.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100% 108/108 [00:00<00:00, 140.73it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.00 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 60 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/59      3.73G     0.1112    0.02991    0.02865         38        640: 100% 22/22 [00:08<00:00,  2.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:02<00:00,  1.55it/s]\n",
            "                   all        108        117    0.00315      0.874     0.0187      0.005\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/59      4.21G    0.08215    0.02859    0.02635         45        640: 100% 22/22 [00:05<00:00,  4.39it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.97it/s]\n",
            "                   all        108        117      0.215      0.313      0.196     0.0856\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/59      4.21G     0.0702    0.02516    0.02139         50        640: 100% 22/22 [00:05<00:00,  4.39it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  4.89it/s]\n",
            "                   all        108        117      0.745      0.235      0.342      0.145\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/59      4.21G    0.06578    0.02395     0.0187         51        640: 100% 22/22 [00:04<00:00,  4.40it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.37it/s]\n",
            "                   all        108        117      0.701      0.265      0.315      0.136\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/59      4.21G    0.06339    0.02209    0.01562         41        640: 100% 22/22 [00:04<00:00,  4.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  4.84it/s]\n",
            "                   all        108        117      0.736      0.295      0.439      0.191\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/59      4.21G    0.05733     0.0211    0.01415         38        640: 100% 22/22 [00:04<00:00,  4.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  4.93it/s]\n",
            "                   all        108        117      0.306      0.572      0.466      0.229\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/59      4.21G     0.0614    0.02119    0.01294         34        640: 100% 22/22 [00:04<00:00,  4.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.21it/s]\n",
            "                   all        108        117      0.377      0.393       0.37      0.178\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/59      4.21G     0.0538    0.02142    0.01484         44        640: 100% 22/22 [00:05<00:00,  4.38it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.27it/s]\n",
            "                   all        108        117      0.309      0.493       0.35      0.176\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/59      4.21G    0.05558    0.02151    0.01169         45        640: 100% 22/22 [00:04<00:00,  4.43it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.22it/s]\n",
            "                   all        108        117       0.39      0.503      0.379      0.181\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/59      4.21G    0.05284    0.02083    0.01189         48        640: 100% 22/22 [00:04<00:00,  4.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.45it/s]\n",
            "                   all        108        117       0.64      0.517      0.557      0.279\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/59      4.21G     0.0522    0.02095    0.01002         46        640: 100% 22/22 [00:04<00:00,  4.49it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.42it/s]\n",
            "                   all        108        117      0.671      0.533      0.587      0.256\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/59      4.21G    0.05319    0.01957   0.009137         47        640: 100% 22/22 [00:04<00:00,  4.41it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.10it/s]\n",
            "                   all        108        117      0.594      0.594      0.595      0.232\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      12/59      4.21G    0.04599    0.01911    0.01188         45        640: 100% 22/22 [00:04<00:00,  4.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.25it/s]\n",
            "                   all        108        117      0.772      0.614      0.695      0.345\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      13/59      4.21G    0.04876    0.02116   0.009204         49        640: 100% 22/22 [00:04<00:00,  4.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.45it/s]\n",
            "                   all        108        117       0.56      0.617      0.604      0.276\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      14/59      4.21G     0.0453    0.01949   0.008345         41        640: 100% 22/22 [00:04<00:00,  4.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.32it/s]\n",
            "                   all        108        117      0.737      0.645      0.706      0.369\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      15/59      4.21G    0.04619    0.01986   0.009598         50        640: 100% 22/22 [00:04<00:00,  4.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.24it/s]\n",
            "                   all        108        117      0.562      0.515      0.552      0.234\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      16/59      4.21G    0.04414    0.01854   0.007457         36        640: 100% 22/22 [00:04<00:00,  4.43it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.30it/s]\n",
            "                   all        108        117      0.691      0.503       0.59      0.337\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      17/59      4.21G    0.04469    0.01836    0.00665         42        640: 100% 22/22 [00:04<00:00,  4.43it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.28it/s]\n",
            "                   all        108        117      0.793      0.605      0.725      0.366\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      18/59      4.21G    0.04715    0.01895   0.007051         40        640: 100% 22/22 [00:04<00:00,  4.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.09it/s]\n",
            "                   all        108        117      0.756      0.668      0.749      0.412\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      19/59      4.21G    0.04177    0.01934   0.007478         44        640: 100% 22/22 [00:04<00:00,  4.43it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.45it/s]\n",
            "                   all        108        117      0.729      0.776      0.775      0.449\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      20/59      4.21G    0.04532    0.01815     0.0073         35        640: 100% 22/22 [00:04<00:00,  4.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.23it/s]\n",
            "                   all        108        117      0.797      0.703       0.77      0.464\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      21/59      4.21G    0.04057    0.01783   0.006657         41        640: 100% 22/22 [00:04<00:00,  4.41it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  4.79it/s]\n",
            "                   all        108        117      0.748      0.679      0.761      0.449\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      22/59      4.21G     0.0373    0.01741    0.00766         41        640: 100% 22/22 [00:04<00:00,  4.42it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.25it/s]\n",
            "                   all        108        117      0.797       0.73      0.781      0.499\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      23/59      4.21G    0.04093    0.01775   0.005181         41        640: 100% 22/22 [00:04<00:00,  4.43it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.39it/s]\n",
            "                   all        108        117      0.795      0.728      0.818      0.501\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      24/59      4.21G    0.03645    0.01732    0.00525         48        640: 100% 22/22 [00:04<00:00,  4.42it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.05it/s]\n",
            "                   all        108        117      0.743      0.792      0.808      0.498\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      25/59      4.21G     0.0401    0.01795   0.006446         40        640: 100% 22/22 [00:05<00:00,  4.37it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.23it/s]\n",
            "                   all        108        117      0.843      0.739      0.823      0.517\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      26/59      4.21G    0.03982    0.01748   0.005351         38        640: 100% 22/22 [00:05<00:00,  4.34it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.35it/s]\n",
            "                   all        108        117      0.793      0.724      0.808      0.451\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      27/59      4.21G    0.03848    0.01732   0.005015         44        640: 100% 22/22 [00:05<00:00,  4.36it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.34it/s]\n",
            "                   all        108        117      0.887       0.78      0.857      0.548\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      28/59      4.21G    0.04005    0.01723   0.007048         49        640: 100% 22/22 [00:04<00:00,  4.41it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.27it/s]\n",
            "                   all        108        117      0.835      0.779      0.835      0.537\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      29/59      4.21G    0.03623    0.01808   0.007823         41        640: 100% 22/22 [00:04<00:00,  4.42it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.46it/s]\n",
            "                   all        108        117      0.821      0.786      0.825      0.542\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      30/59      4.21G    0.03795    0.01794   0.005544         39        640: 100% 22/22 [00:05<00:00,  4.39it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.37it/s]\n",
            "                   all        108        117      0.867      0.761      0.846      0.512\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      31/59      4.21G    0.03845    0.01711   0.005151         36        640: 100% 22/22 [00:04<00:00,  4.43it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.29it/s]\n",
            "                   all        108        117       0.85      0.762      0.835      0.546\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      32/59      4.21G    0.03812     0.0167   0.006385         45        640: 100% 22/22 [00:05<00:00,  4.36it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.28it/s]\n",
            "                   all        108        117      0.796      0.795      0.836      0.536\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      33/59      4.21G    0.03448    0.01657   0.007505         38        640: 100% 22/22 [00:04<00:00,  4.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.01it/s]\n",
            "                   all        108        117      0.812      0.747      0.832      0.556\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      34/59      4.21G    0.03563    0.01581   0.005483         41        640: 100% 22/22 [00:04<00:00,  4.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.25it/s]\n",
            "                   all        108        117      0.824      0.763      0.856       0.55\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      35/59      4.21G    0.03491     0.0165   0.005056         46        640: 100% 22/22 [00:04<00:00,  4.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.17it/s]\n",
            "                   all        108        117      0.882      0.737      0.805      0.552\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      36/59      4.21G    0.03606    0.01565    0.00402         38        640: 100% 22/22 [00:04<00:00,  4.41it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.31it/s]\n",
            "                   all        108        117      0.817      0.669       0.76      0.501\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      37/59      4.21G    0.03251    0.01585   0.003073         35        640: 100% 22/22 [00:05<00:00,  4.33it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.24it/s]\n",
            "                   all        108        117      0.826      0.751      0.812      0.562\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      38/59      4.21G    0.03456    0.01632   0.004591         48        640: 100% 22/22 [00:04<00:00,  4.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.04it/s]\n",
            "                   all        108        117      0.798      0.804      0.828      0.549\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      39/59      4.21G    0.03351    0.01581   0.004094         45        640: 100% 22/22 [00:05<00:00,  4.33it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.48it/s]\n",
            "                   all        108        117      0.932      0.774      0.884      0.589\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      40/59      4.21G    0.03122     0.0159   0.004869         48        640: 100% 22/22 [00:05<00:00,  4.39it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.43it/s]\n",
            "                   all        108        117      0.864      0.808      0.888      0.617\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      41/59      4.21G    0.03415    0.01648   0.004536         47        640: 100% 22/22 [00:05<00:00,  4.35it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.17it/s]\n",
            "                   all        108        117      0.899      0.807      0.899       0.63\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      42/59      4.21G    0.03152     0.0157   0.002953         48        640: 100% 22/22 [00:04<00:00,  4.43it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.25it/s]\n",
            "                   all        108        117       0.92      0.803      0.891      0.623\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      43/59      4.21G    0.03138    0.01612   0.005008         50        640: 100% 22/22 [00:05<00:00,  4.32it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.40it/s]\n",
            "                   all        108        117      0.838      0.803      0.857      0.617\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      44/59      4.21G    0.02966    0.01493   0.002847         39        640: 100% 22/22 [00:04<00:00,  4.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.06it/s]\n",
            "                   all        108        117      0.812      0.755      0.834       0.61\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      45/59      4.21G    0.03248    0.01515   0.002749         47        640: 100% 22/22 [00:05<00:00,  4.35it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.11it/s]\n",
            "                   all        108        117      0.863      0.786      0.874      0.637\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      46/59      4.21G    0.02902    0.01406   0.002344         32        640: 100% 22/22 [00:04<00:00,  4.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.19it/s]\n",
            "                   all        108        117      0.896      0.813      0.896      0.633\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      47/59      4.21G    0.02897    0.01495   0.002818         43        640: 100% 22/22 [00:05<00:00,  4.37it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.42it/s]\n",
            "                   all        108        117      0.895      0.824      0.877       0.62\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      48/59      4.21G    0.03032    0.01484   0.002788         43        640: 100% 22/22 [00:04<00:00,  4.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.32it/s]\n",
            "                   all        108        117      0.887      0.794      0.862      0.622\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      49/59      4.21G    0.02991    0.01392   0.003035         43        640: 100% 22/22 [00:04<00:00,  4.40it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.45it/s]\n",
            "                   all        108        117      0.892      0.806      0.865      0.619\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      50/59      4.21G    0.02941    0.01453   0.002642         32        640: 100% 22/22 [00:04<00:00,  4.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.35it/s]\n",
            "                   all        108        117      0.828      0.833      0.862      0.607\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      51/59      4.21G      0.027    0.01452   0.002781         47        640: 100% 22/22 [00:05<00:00,  4.35it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.31it/s]\n",
            "                   all        108        117      0.865      0.817      0.875      0.622\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      52/59      4.21G    0.02752    0.01426   0.002038         38        640: 100% 22/22 [00:04<00:00,  4.41it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.09it/s]\n",
            "                   all        108        117      0.907      0.786      0.877      0.634\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      53/59      4.21G    0.02669    0.01441   0.002004         38        640: 100% 22/22 [00:04<00:00,  4.42it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.43it/s]\n",
            "                   all        108        117      0.891      0.785      0.872      0.659\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      54/59      4.21G    0.02838    0.01383   0.002445         46        640: 100% 22/22 [00:05<00:00,  4.34it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.19it/s]\n",
            "                   all        108        117      0.926      0.773      0.878      0.661\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      55/59      4.21G    0.02948    0.01464   0.002586         43        640: 100% 22/22 [00:04<00:00,  4.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.38it/s]\n",
            "                   all        108        117      0.934       0.79      0.884      0.657\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      56/59      4.21G    0.02625     0.0136   0.002043         42        640: 100% 22/22 [00:04<00:00,  4.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  4.99it/s]\n",
            "                   all        108        117      0.896      0.835       0.88      0.642\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      57/59      4.21G    0.02539    0.01366   0.001762         40        640: 100% 22/22 [00:05<00:00,  4.38it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.37it/s]\n",
            "                   all        108        117      0.912      0.843      0.885      0.659\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      58/59      4.21G    0.02662    0.01355   0.002232         44        640: 100% 22/22 [00:05<00:00,  4.36it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.34it/s]\n",
            "                   all        108        117      0.916      0.828      0.888      0.659\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      59/59      4.21G    0.02689    0.01467   0.001813         39        640: 100% 22/22 [00:04<00:00,  4.41it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.35it/s]\n",
            "                   all        108        117      0.943      0.812      0.895      0.666\n",
            "\n",
            "60 epochs completed in 0.104 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  2.44it/s]\n",
            "                   all        108        117      0.942      0.812      0.895      0.666\n",
            "                   gun        108         51      0.975       0.76      0.838      0.599\n",
            "           machine gun        108         66       0.91      0.864      0.952      0.733\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15glLzbQx5u0"
      },
      "source": [
        "# 4. Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comet Logging and Visualization ðŸŒŸ NEW\n",
        "[Comet](https://bit.ly/yolov5-readme-comet) is now fully integrated with YOLOv5. Track and visualize model metrics in real time, save your hyperparameters, datasets, and model checkpoints, and visualize your model predictions with [Comet Custom Panels](https://bit.ly/yolov5-colab-comet-panels)! Comet makes sure you never lose track of your work and makes it easy to share results and collaborate across teams of all sizes! \n",
        "\n",
        "Getting started is easy:\n",
        "```shell\n",
        "pip install comet_ml  # 1. install\n",
        "export COMET_API_KEY=<Your API Key>  # 2. paste API key\n",
        "python train.py --img 640 --epochs 3 --data coco128.yaml --weights yolov5s.pt  # 3. train\n",
        "```\n",
        "\n",
        "To learn more about all of the supported Comet features for this integration, check out the [Comet Tutorial](https://github.com/ultralytics/yolov5/tree/master/utils/loggers/comet). If you'd like to learn more about Comet, head over to our [documentation](https://bit.ly/yolov5-colab-comet-docs). Get started by trying out the Comet Colab Notebook:\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1RG0WOQyxlDlo5Km8GogJpIEJlg_5lyYO?usp=sharing)\n",
        "\n",
        "<img width=\"1920\" alt=\"yolo-ui\" src=\"https://user-images.githubusercontent.com/7529846/187608607-ff89c3d5-1b8b-4743-a974-9275301b0524.png\">"
      ],
      "metadata": {
        "id": "nWOsI5wJR1o3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ClearML Logging and Automation ðŸŒŸ NEW\n",
        "\n",
        "[ClearML](https://cutt.ly/yolov5-notebook-clearml) is completely integrated into YOLOv5 to track your experimentation, manage dataset versions and even remotely execute training runs. To enable ClearML (check cells above):\n",
        "\n",
        "- `pip install clearml`\n",
        "- run `clearml-init` to connect to a ClearML server (**deploy your own [open-source server](https://github.com/allegroai/clearml-server)**, or use our [free hosted server](https://cutt.ly/yolov5-notebook-clearml))\n",
        "\n",
        "You'll get all the great expected features from an experiment manager: live updates, model upload, experiment comparison etc. but ClearML also tracks uncommitted changes and installed packages for example. Thanks to that ClearML Tasks (which is what we call experiments) are also reproducible on different machines! With only 1 extra line, we can schedule a YOLOv5 training task on a queue to be executed by any number of ClearML Agents (workers).\n",
        "\n",
        "You can use ClearML Data to version your dataset and then pass it to YOLOv5 simply using its unique ID. This will help you keep track of your data without adding extra hassle. Explore the [ClearML Tutorial](https://github.com/ultralytics/yolov5/tree/master/utils/loggers/clearml) for details!\n",
        "\n",
        "<a href=\"https://cutt.ly/yolov5-notebook-clearml\">\n",
        "<img alt=\"ClearML Experiment Management UI\" src=\"https://github.com/thepycoder/clearml_screenshots/raw/main/scalars.jpg\" width=\"1280\"/></a>"
      ],
      "metadata": {
        "id": "Lay2WsTjNJzP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLI1JmHU7B0l"
      },
      "source": [
        "## Weights & Biases Logging\n",
        "\n",
        "[Weights & Biases](https://wandb.ai/site?utm_campaign=repo_yolo_notebook) (W&B) is integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration for teams. To enable W&B `pip install wandb`, and then train normally (you will be guided through setup on first use). \n",
        "\n",
        "During training you will see live updates at [https://wandb.ai/home](https://wandb.ai/home?utm_campaign=repo_yolo_notebook), and you can create and share detailed [Reports](https://wandb.ai/glenn-jocher/yolov5_tutorial/reports/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY) of your results. For more information see the [YOLOv5 Weights & Biases Tutorial](https://github.com/ultralytics/yolov5/issues/1289).  \n",
        "\n",
        "<a href=\"https://wandb.ai/glenn-jocher/yolov5_tutorial\">\n",
        "<img alt=\"Weights & Biases dashboard\" src=\"https://user-images.githubusercontent.com/26833433/182482859-288a9622-4661-48db-99de-650d1dead5c6.jpg\" width=\"1280\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WPvRbS5Swl6"
      },
      "source": [
        "## Local Logging\n",
        "\n",
        "Training results are automatically logged with [Tensorboard](https://www.tensorflow.org/tensorboard) and [CSV](https://github.com/ultralytics/yolov5/pull/4148) loggers to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc.\n",
        "\n",
        "This directory contains train and val statistics, mosaics, labels, predictions and augmentated mosaics, as well as metrics and charts including precision-recall (PR) curves and confusion matrices. \n",
        "\n",
        "<img alt=\"Local logging results\" src=\"https://user-images.githubusercontent.com/26833433/183222430-e1abd1b7-782c-4cde-b04d-ad52926bf818.jpg\" width=\"1280\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zelyeqbyt3GD"
      },
      "source": [
        "# Environments\n",
        "\n",
        "YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n",
        "\n",
        "- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n",
        "- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart)\n",
        "- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/AWS-Quickstart)\n",
        "- **Docker Image**. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qu7Iesl0p54"
      },
      "source": [
        "# Status\n",
        "\n",
        "![YOLOv5 CI](https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg)\n",
        "\n",
        "If this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEijrePND_2I"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "Additional content below for PyTorch Hub, CI, reproducing results, profiling speeds, VOC training, classification training and TensorRT example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMusP4OAxFu6"
      },
      "source": [
        "import torch\n",
        "\n",
        "# PyTorch Hub Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5n - yolov5x6, custom\n",
        "\n",
        "# Images\n",
        "img = 'https://ultralytics.com/images/zidane.jpg'  # or file, Path, PIL, OpenCV, numpy, list\n",
        "\n",
        "# Inference\n",
        "results = model(img)\n",
        "\n",
        "# Results\n",
        "results.print()  # or .show(), .save(), .crop(), .pandas(), etc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGH0ZjkGjejy"
      },
      "source": [
        "# YOLOv5 CI\n",
        "%%shell\n",
        "rm -rf runs  # remove runs/\n",
        "m=yolov5n  # official weights\n",
        "b=runs/train/exp/weights/best  # best.pt checkpoint\n",
        "python train.py --imgsz 64 --batch 32 --weights $m.pt --cfg $m.yaml --epochs 1 --device 0  # train\n",
        "for d in 0 cpu; do  # devices\n",
        "  for w in $m $b; do  # weights\n",
        "    python val.py --imgsz 64 --batch 32 --weights $w.pt --device $d  # val\n",
        "    python detect.py --imgsz 64 --weights $w.pt --device $d  # detect\n",
        "  done\n",
        "done\n",
        "python hubconf.py --model $m  # hub\n",
        "python models/tf.py --weights $m.pt  # build TF model\n",
        "python models/yolo.py --cfg $m.yaml  # build PyTorch model\n",
        "python export.py --weights $m.pt --img 64 --include torchscript  # export"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcKoSIK2WSzj"
      },
      "source": [
        "# Reproduce\n",
        "for x in (f'yolov5{x}' for x in 'nsmlx'):\n",
        "  !python val.py --weights {x}.pt --data coco.yaml --img 640 --task speed  # speed\n",
        "  !python val.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.001 --iou 0.65  # mAP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gogI-kwi3Tye"
      },
      "source": [
        "# Profile\n",
        "from utils.torch_utils import profile\n",
        "\n",
        "m1 = lambda x: x * torch.sigmoid(x)\n",
        "m2 = torch.nn.SiLU()\n",
        "results = profile(input=torch.randn(16, 3, 640, 640), ops=[m1, m2], n=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSgFCAcMbk1R"
      },
      "source": [
        "# VOC\n",
        "for b, m in zip([64, 64, 64, 32, 16], [f'yolov5{x}' for x in 'nsmlx']):  # batch, model\n",
        "  !python train.py --batch {b} --weights {m}.pt --data VOC.yaml --epochs 50 --img 512 --hyp hyp.VOC.yaml --project VOC --name {m} --cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification train\n",
        "for m in [*(f'yolov5{x}-cls.pt' for x in 'nsmlx'), 'resnet50.pt', 'resnet101.pt', 'efficientnet_b0.pt', 'efficientnet_b1.pt']:\n",
        "  for d in 'mnist', 'fashion-mnist', 'cifar10', 'cifar100', 'imagenette160', 'imagenette320', 'imagenette', 'imagewoof160', 'imagewoof320', 'imagewoof':\n",
        "    !python classify/train.py --model {m} --data {d} --epochs 10 --project YOLOv5-cls --name {m}-{d}"
      ],
      "metadata": {
        "id": "UWGH7H6yakVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification val\n",
        "!bash data/scripts/get_imagenet.sh --val  # download ImageNet val split (6.3G - 50000 images)\n",
        "!python classify/val.py --weights yolov5m-cls.pt --data ../datasets/imagenet --img 224  # validate"
      ],
      "metadata": {
        "id": "yYgOiFNHZx-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate on COCO test. Zip results.json and submit to eval server at https://competitions.codalab.org/competitions/20794\n",
        "!bash data/scripts/get_coco.sh --test  # download COCO test-dev2017 (7G - 40000 images, test 20000)\n",
        "!python val.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65 --half --task test"
      ],
      "metadata": {
        "id": "aq4DPWGu0Bl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTRwsvA9u7ln"
      },
      "source": [
        "# TensorRT \n",
        "!pip install -U nvidia-tensorrt --index-url https://pypi.ngc.nvidia.com  # install\n",
        "!python export.py --weights yolov5s.pt --include engine --imgsz 640 --device 0  # export\n",
        "!python detect.py --weights yolov5s.engine --imgsz 640 --device 0  # inference"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}